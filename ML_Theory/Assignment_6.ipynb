{
 "cells": [
  {
   "cell_type": "raw",
   "id": "83461626",
   "metadata": {},
   "source": [
    "ques 1) A training model is a dataset that is used to train an ML algorithm. It consists of the sample output data and the corresponding sets of input data that have an influence on the output. The training model is used to run the input data through the algorithm to correlate the processed output against the sample output."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eafc8dc0",
   "metadata": {},
   "source": [
    "ques 2) The No Free Lunch Theorem is often thrown around in the field of optimization and machine learning, often with little understanding of what it means or implies. The theorem states that all optimization algorithms perform equally well when their performance is averaged across all possible problems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0c4cfc4",
   "metadata": {},
   "source": [
    "ques 3) Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed29d3d4",
   "metadata": {},
   "source": [
    "ques 4) The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It can be used to estimate summary statistics such as the mean or standard deviation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea75dc2c",
   "metadata": {},
   "source": [
    "ques 5) It basically tells you how much better your classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class. Cohen's kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless"
   ]
  },
  {
   "cell_type": "raw",
   "id": "314975c8",
   "metadata": {},
   "source": [
    "ques 6) Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model . To better understand this definition lets take a step back into ultimate goal of machine learning and model building."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6c33da5",
   "metadata": {},
   "source": [
    "ques 7) A descriptive model describes a system or other entity and its relationship to its environment. It is generally used to help specify and/or understand what the system is, what it does, and how it does it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ed7009a",
   "metadata": {},
   "source": [
    "ques 8) There are 3 main metrics for model evaluation in regression:\n",
    "    R Square/Adjusted R Square.\n",
    "    Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
    "    Mean Absolute Error(MAE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb2a3732",
   "metadata": {},
   "source": [
    "ques 9) A descriptive model will exploit the past data that are stored in databases and provide you with the accurate report. In a Predictive model, it identifies patterns found in past and transactional data to find risks and future outcomes.\n",
    "\n",
    "This situation where any given model is performing too well on the training data but the performance drops significantly over the test set is called an overfitting model. On the other hand, if the model is performing poorly over the test and the train set, then we call that an underfitting model.\n",
    "\n",
    "\n",
    "Cross validation splits the available dataset to create multiple datasets, and Bootstrapping method uses the original dataset to create multiple datasets after resampling with replacement."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbd2550b",
   "metadata": {},
   "source": [
    "ques 10) LOOCV(Leave One Out Cross-Validation) is a type of cross-validation approach in which each observation is considered as the validation set and the rest (N-1) observations are considered as the training set\n",
    "\n",
    "Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7ae7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
