{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques1)Supervised learning, as the name indicates, has the presence of a supervisor as a teacher. Basically supervised learning is when we teach or train the machine using data that is well labeled. Which means some data is already tagged with the correct answer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques2) Some of applications are :\n",
    "    Identifying Diseases and Diagnosis\n",
    "    Drug Discovery and Manufacturing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques3) Supervised learning examples are:\n",
    "       Store Sales prediction\n",
    "       Spam Prediction\n",
    "       Churn Prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques4) Regression - Regression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).\n",
    "\n",
    "A continuous output variable is a real-value, such as an integer or floating point value. These are often quantities, such as amounts and sizes.\n",
    "\n",
    "Classification - Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y).\n",
    "\n",
    "The output variables are often called labels or categories. The mapping function predicts the class or category for a given observation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques5) Different types of classification algorithms are:\n",
    "       Logistic Regression\n",
    "        Decision tree\n",
    "        Ensemble Techniques\n",
    "        Naive Bayes\n",
    "        etc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques6) Support Vector Machine” (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. Support Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier that best segregates the two classes (hyper-plane/ line)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques7) In cost-sensitive learning instead of each instance being either correctly or incorrectly classified, each class (or instance) is given a misclassification cost. Thus, instead of trying to optimize the accuracy, the problem is then to minimize the total misclassification cost."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques8) Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques9) SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques10) Effectiveness of SVM on:\n",
    "    A) Selection of Kernel\n",
    "    B) Kernel Parameters\n",
    "    C) Soft Margin Parameter C"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques11) SVM works relatively well when there is a clear margin of separation between classes. SVM is more effective in high dimensional spaces. SVM is effective in cases where the number of dimensions is greater than the number of samples. SVM is relatively memory efficient."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques12) SVM algorithm is not suitable for large data sets. SVM does not perform very well when the data set has more noise i.e. target classes are overlapping. In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques13) The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques14) Some advantages of KNN are:\n",
    "     Quick calculation time.\n",
    "    Simple algorithm – to interpret.\n",
    "    Versatile – useful for regression and classification.\n",
    "    High accuracy – you do not need to compare with better-supervised learning models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques15)  Some disadvantages of KNN are: \n",
    "    Accuracy depends on the quality of the data.\n",
    "    With large data, the prediction stage might be slow.\n",
    "    Sensitive to the scale of the data and irrelevant features.\n",
    "    Require high memory – need to store all of the training data.\n",
    "    Given that it stores all of the training, it can be computationally expensive."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques16) A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions. Tree models where the target variable can take a finite set of values are called classification trees and target variable can take continuous values (numbers) are called regression trees."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques17) A decision tree is a structure that includes a root node, branches, and leaf nodes. Each internal node denotes a test on an attribute, each branch denotes the outcome of a test, and each leaf node holds a class label. Each internal node represents a test on an attribute. Each leaf node represents a class."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques18) Entropy helps us to build an appropriate decision tree for selecting the best splitter. Entropy can be defined as a measure of the purity of the sub split. Entropy always lies between 0 to 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques19) Information gain helps to determine the order of attributes in the nodes of a decision tree. The main node is referred to as the parent node, whereas sub-nodes are known as child nodes. We can use information gain to determine how good the splitting of nodes in a decision tree."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques20) A significant advantage of a decision tree is that it forces the consideration of all possible outcomes of a decision and traces each path to a conclusion. It creates a comprehensive analysis of the consequences along each branch and identifies decision nodes that need further analysis."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques21) Disadvantages of decision trees: They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree. They are often relatively inaccurate. Many other predictors perform better with similar data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ques22) The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
