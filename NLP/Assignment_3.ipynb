{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f67c1b5c",
   "metadata": {},
   "source": [
    "Ans.1: The fundamental feature of a Recurrent Neural Network (RNN) is that the network contains at least one feed-back connection, so the activations can flow round in a loop. That enables the networks to do temporal processing and learn sequences, e.g., perform sequence recognition/reproduction or temporal association/prediction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9ea7f47",
   "metadata": {},
   "source": [
    "Ans.2: Backpropagation refers to two things:\n",
    "\n",
    "The mathematical method used to calculate derivatives and an application of the derivative chain rule.\n",
    "The training algorithm for updating network weights to minimize error.\n",
    "\n",
    "The goal of the backpropagation training algorithm is to modify the weights of a neural network in order to minimize the error of the network outputs compared to some expected output in response to corresponding inputs.\n",
    "It is a supervised learning algorithm that allows the network to be corrected with regard to the specific errors made."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a018a05b",
   "metadata": {},
   "source": [
    "Ans.3: Vanishing –\n",
    "As the backpropagation algorithm advances downwards(or backward) from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged. As a result, the gradient descent never converges to the optimum. This is known as the vanishing gradients problem.\n",
    "Exploding –\n",
    "On the contrary, in some cases, the gradients keep on getting larger and larger as the backpropagation algorithm progresses. This, in turn, causes very large weight updates and causes the gradient descent to diverge. This is known as the exploding gradients problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "261387c1",
   "metadata": {},
   "source": [
    "Ans.4: Long Short Term Memory Network is an advanced RNN, a sequential network, that allows information to persist. It is capable of handling the vanishing gradient problem faced by RNN. A recurrent neural network is also known as RNN is used for persistent memory.\n",
    "\n",
    "Let’s say while watching a video you remember the previous scene or while reading a book you know what happened in the earlier chapter. Similarly RNNs work, they remember the previous information and use it for processing the current input. The shortcoming of RNN is, they can not remember Long term dependencies due to vanishing gradient. LSTMs are explicitly designed to avoid long-term dependency problems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f17a9dc",
   "metadata": {},
   "source": [
    "Ans.5: GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results. \n",
    "To solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "faff80d7",
   "metadata": {},
   "source": [
    "Ans.6: Peephole connections were originally introduced by Gers and Schmidhuber in 2000 to help LSTMs learn precise timings. \n",
    "LSTM augmented by “peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b9f78b3",
   "metadata": {},
   "source": [
    "Ans.7: Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. The input sequence is fed in normal time order for one network, and in reverse time order for another. The outputs of the two networks are usually concatenated at each time step, though there are other options, e.g. summation.\n",
    "This structure allows the networks to have both backward and forward information about the sequence at every time step."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6be267a3",
   "metadata": {},
   "source": [
    "Ans.8: First equation is for Input Gate which tells us that what new information we’re going to store in the cell state(that we will see below).\n",
    "Second is for the forget gate which tells the information to throw away from the cell state.\n",
    "Third one is for the output gate which is used to provide the activation to the final output of the lstm block at timestamp ‘t’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1b34c",
   "metadata": {},
   "source": [
    "![Equations](https://miro.medium.com/max/672/1*T9YrMjrpu5UnVUFO50WYrA.png)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2feb9142",
   "metadata": {},
   "source": [
    "Ans.9: A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a35fed7a",
   "metadata": {},
   "source": [
    "Ans.10: A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
