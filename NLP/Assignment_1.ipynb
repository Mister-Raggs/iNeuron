{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e6715df6",
   "metadata": {},
   "source": [
    "Ans.1: ne hot encoding is a process of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. One hot encoding is a crucial part of feature engineering for machine learning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbd8f3b4",
   "metadata": {},
   "source": [
    "Ans.2: A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e302f24c",
   "metadata": {},
   "source": [
    "Ans.3: A bag-of-n-grams model is a way to represent a document, similar to a bag-of-words model.A bag-of-n-grams model represents a text document as an unordered collection of its n-grams.\n",
    "A bag-of-n-grams model has the simplicity of the bag-of-words model, but allows the preservation of more word locality information."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cb73acf",
   "metadata": {},
   "source": [
    "Ans.4: TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a146e6b4",
   "metadata": {},
   "source": [
    "Ans.5: Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. \n",
    "In speech recognition, it’s the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning. But the limitation of word embeddings is that the words need to have been seen before in the training data.\n",
    "When a word that’s not in the training set occurs in real data, this causes a problem. There are various techniques to avoid a zero-probability occurrence including smoothing and replacing the word a synonym."
   ]
  },
  {
   "cell_type": "raw",
   "id": "40810b8a",
   "metadata": {},
   "source": [
    "Ans.6: A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
    "It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e937e5e1",
   "metadata": {},
   "source": [
    "Ans.7: The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are contextually accurate. Let us consider an example for understanding this. Consider the sentence: ‘It is a pleasant day’ and the word ‘pleasant’ goes as input to the neural network. We are trying to predict the word ‘day’ here. We will use the one-hot encoding for the input words and measure the error rates with the one-hot encoded target word. Doing this will help us predict the output based on the word with least error. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c25c983",
   "metadata": {},
   "source": [
    "Ans.8: Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It’s reverse of CBOW algorithm. Here, target word is input while context words are output. As there is more than one context word to be predicted which makes this problem difficult."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eb5af68",
   "metadata": {},
   "source": [
    "Ans.9: GloVe is a word vector technique that rode the wave of word vectors after a brief silence. Just to refresh, word vectors put words to a nice vector space, where similar words cluster together and different words repel. The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors. But keep in mind that there’s quite a bit of synergy between the GloVe and Word2vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
