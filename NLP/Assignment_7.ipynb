{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5cd3e025",
   "metadata": {},
   "source": [
    "Ans.1: BERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack. These are more than the Transformer architecture described in the original paper (6 encoder layers). BERT architectures (BASE and LARGE) also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the Transformer architecture suggested in the original paper. It contains 512 hidden units and 8 attention heads. BERTBASE contains 110M parameters while BERTLARGE has 340M parameters."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88f31ebe",
   "metadata": {},
   "source": [
    "Ans.2: MLM consists of giving BERT a sentence and optimizing the weights inside BERT to output the same sentence on the other side.\n",
    "So we input a sentence and ask that BERT outputs the same sentence.\n",
    "However, before we actually give BERT that input sentence — we mask a few tokens.\n",
    "In this image, before passing our tokens into BERT — we have masked the lincoln token, replacing it with [MASK].\n",
    "So we’re actually inputting an incomplete sentence and asking BERT to complete it for us."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bac3c8a",
   "metadata": {},
   "source": [
    "Ans.3: In BERT, a Next Sentence Prediction (NSP) language task is added to the training process.\n",
    "This task essentially boils down to the following question: Given sentences A and B, is B the next sentence for A?\n",
    "For example, if we have these pairs of sentences:\n",
    "Jim went to the store. He bought a pair of shoes.\n",
    "Peter cooked a meal. The garage was empty.\n",
    "Clearly, B follows A in (1), but not in (2).\n",
    "That’s the goal of the NSP objective."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3148bc03",
   "metadata": {},
   "source": [
    "Ans.4: MCC takes into account all four values in the confusion matrix, and a high value (close to 1) means that both classes are predicted well, even if one class is disproportionately under- (or over-) represented."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cf80cff",
   "metadata": {},
   "source": [
    "Ans.5: For binary classification, there is another (and arguably more elegant) solution: treat the true class and the predicted class as two (binary) variables, and compute their correlation coefficient (in a similar way to computing correlation coefficient between any two variables). The higher the correlation between true and predicted values, the better the prediction. This is the phi-coefficient (φ), rechristened Matthews Correlation Coefficient (MCC) when applied to classifiers."
   ]
  },
  {
   "cell_type": "raw",
   "id": "848a7da9",
   "metadata": {},
   "source": [
    "Ans.6: Semantic role labeling aims to model the predicate-argument structure of a sentence and is often described as answering \"Who did what to whom\". BIO notation is typically used for semantic role labeling.\n",
    "\n",
    "Example:\n",
    "Housing \tstarts \tare \texpected \tto \tquicken \ta \tbit \tfrom \tAugust’s \tpace\n",
    "B-ARG1 \tI-ARG1 \tO \tO \tO \tV \tB-ARG2 \tI-ARG2 \tB-ARG3 \tI-ARG3 \tI-ARG3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d6f6ce1",
   "metadata": {},
   "source": [
    "Ans.7: There are two steps in BERT: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.\n",
    "Fine-tuning works on labelled data while pretraining has to take the time to determine the labels for fine-tuning."
   ]
  },
  {
   "cell_type": "raw",
   "id": "02388a82",
   "metadata": {},
   "source": [
    "Ans.8: Textual Entailment Recognition has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Text Summarization. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
