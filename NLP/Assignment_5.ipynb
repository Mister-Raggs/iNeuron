{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f8e139e7",
   "metadata": {},
   "source": [
    "Ans.1: Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n",
    "\n",
    "\"the cat sat on the mat\" -> [Seq2Seq model] -> \"le chat etait assis sur le tapis\"\n",
    "\n",
    "This can be used for machine translation or for free-from question answering (generating a natural language answer given a natural language question) -- in general, it is applicable any time you need to generate text."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c71e6a65",
   "metadata": {},
   "source": [
    "Ans.2: RNNs enable modelling time-dependent and sequential data tasks, such as stock market prediction, machine translation, text generation and many more.\n",
    "However, RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done."
   ]
  },
  {
   "cell_type": "raw",
   "id": "62c2b833",
   "metadata": {},
   "source": [
    "Ans.3: Gradient clipping involves forcing the gradient values (element-wise) to a specific minimum or maximum value if the gradient exceeded an expected range.\n",
    "Together, these methods are often simply referred to as “gradient clipping.”"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6063a24",
   "metadata": {},
   "source": [
    "Ans.4: The attention mechanism was introduced to address the bottleneck problem that arises with the use of a fixed-length encoding vector, where the decoder would have limited access to the information provided by the input. This is thought to become especially problematic for long and/or complex sequences, where the dimensionality of their representation would be forced to be the same as for shorter or simpler sequences."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2880ccd0",
   "metadata": {},
   "source": [
    "Ans.5: Conditional Random Fields is a class of discriminative models best suited to prediction tasks where contextual information or state of the neighbors affect the current prediction. CRFs find their applications in named entity recognition, part of speech tagging, gene prediction, noise reduction and object detection problems, to name a few."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fea94fb",
   "metadata": {},
   "source": [
    "Ans.6: Self Attention, also called intra Attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8515b12",
   "metadata": {},
   "source": [
    "Ans.7: The Bahdanau attention mechanism has inherited its name from the first author of the paper in which it was published. \n",
    "It follows the work of Cho et al. (2014) and Sutskever et al. (2014), who had also employed an RNN encoder-decoder framework for neural machine translation, specifically by encoding a variable-length source sentence into a fixed-length vector. The latter would then be decoded into a variable-length target sentence. \n",
    "Bahdanau et al. (2014) argue that this encoding of a variable-length input into a fixed-length vector squashes the information of the source sentence, irrespective of its length, causing the performance of a basic encoder-decoder model to deteriorate rapidly with an increasing length of the input sentence. The approach they propose, on the other hand, replaces the fixed-length vector with a variable-length one, to improve the translation performance of the basic encoder-decoder model. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0425da8f",
   "metadata": {},
   "source": [
    "Ans.8: A language model is basically a probability distribution over words or word sequences. In practice, a language model gives the probability of a certain word sequence being “valid”. Validity in this context does not refer to grammatical validity at all. It means that it resembles how people speak (or, to be more precise, write) — which is what the language model learns. This is an important point: there is no magic to a language model (like other machine learning models, particularly deep neural networks), it is “just” a tool to incorporate abundant information in a concise manner that is reusable in an out-of-sample context."
   ]
  },
  {
   "cell_type": "raw",
   "id": "749c5a1b",
   "metadata": {},
   "source": [
    "Ans.9: Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4462c49d",
   "metadata": {},
   "source": [
    "Ans.10: BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation, the better it is\" – this is the central idea behind BLEU.[1] BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
