{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4799df4c",
   "metadata": {},
   "source": [
    "Ans.1: In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length.\n",
    "Applications are speech recognition, machine translation, image captioning and question answering.\n",
    "\n",
    "Sequence To Vector To Sequence RNN\n",
    "It is like a encoder-decoder model which first we have a temporal information and we condense it down to a vector which we can call it a “concept vector” that is a vector representing the concept of that expanded version then we expand that concept vector to the output that we want. The best example of these networks are sentence translation models.\n",
    "\n",
    "Vector To Sequence RNN\n",
    "The RNN model takes a single vector as input and produces a sequence as output. An example of these models can be image to sentence model, which takes an image(consider it as a vector) and then produces a sentence to describe that image."
   ]
  },
  {
   "cell_type": "raw",
   "id": "79eb4cb4",
   "metadata": {},
   "source": [
    "Ans.3: A video consists of an ordered sequence of frames. Each frame contains spatial information, and the sequence of those frames contains temporal information. To model both of these aspects, we use a hybrid architecture that consists of convolutions (for spatial processing) as well as recurrent layers (for temporal processing). Specifically, we'll use a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN) consisting of GRU layers. This kind of hybrid architecture is popularly known as a CNN-RNN."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfa3e2d3",
   "metadata": {},
   "source": [
    "Ans.4: Static\n",
    "Internally, tf.nn.rnn creates an unrolled graph for a fixed RNN length. That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified.\n",
    "\n",
    "Dynamic\n",
    "tf.nn.dynamic_rnn solves this. It uses a tf.While loop to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c604567",
   "metadata": {},
   "source": [
    "Ans.5: Just do not specify the timespan dimension when building LSTM. Alternative to this is sequence padding."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b6af5f3",
   "metadata": {},
   "source": [
    "Ans.6: \tmodel = multi_gpu_model(model, gpus=G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
