{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5c81afb9",
   "metadata": {},
   "source": [
    "Ans.1: Corpora is a group presenting multiple collections of text documents. A single collection is called corpus. One such famous corpus is the Gutenberg Corpus which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ce65d4e",
   "metadata": {},
   "source": [
    "Ans.2: The smallest individual unit in a program is known as a token. There are five types of tokens allowed in Python. They are :\n",
    "Keywords : for, del, elif, else etc.\n",
    "Identifiers : Variable names like balance, class names like Vehicle etc\n",
    "Literals : String, Numeric, Boolean like ‘abcd’, None etc\n",
    "Operators : Unary, Binary, Bitwise like ‘+’, ‘&’, ‘^’ etc\n",
    "Punctuators : Symbols like ‘#’, ‘(‘, ‘[‘, ‘=’ etc."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ea6dd2c",
   "metadata": {},
   "source": [
    "Ans.3: A 1-gram (or unigram) is a one-word sequence.\n",
    "\"I love butter chicken\"\n",
    "For the above sentence, the unigrams would simply be: “I”, “love”, “butter\", or \"chicken\"\n",
    "A 2-gram (or bigram) is a two-word sequence of words, like “I love”, “love butter”, or “butter chicken”. \n",
    "And a 3-gram (or trigram) is a three-word sequence of words like “I love butter”, “love butter chicken\"."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3151d3ba",
   "metadata": {},
   "source": [
    "Ans.4: We make a function inputs two parameters, namely, text and ngram which refer to the text data for which we want to generate a given number of n-grams and the number of grams to be generated respectively. Firstly, word tokenization is done where the stop words are ignored and the remaining words are retained. From the example section, you must have been clear on how to manually generate n-grams for a given text. We have coded the very same logic in the function generate_N_grams() above. It will thus consider n words at a time from the text where n is given by the value of the ngram parameter of the function."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d662087",
   "metadata": {},
   "source": [
    "Ans.5: Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words."
   ]
  },
  {
   "cell_type": "raw",
   "id": "37596b25",
   "metadata": {},
   "source": [
    "Ans.6: Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\"\n",
    "Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e61b7bef",
   "metadata": {},
   "source": [
    "Ans.7: Tagging is a kind of classification that may be defined as the automatic assignment of description to the tokens. Here the descriptor is called tag, which may represent one of the part-of-speech, semantic information and so on.\n",
    "As for Part-of-Speech (PoS) tagging, then it may be defined as the process of assigning one of the parts of speech to the given word. It is generally called POS tagging. In simple words, we can say that POS tagging is a task of labelling each word in a sentence with its appropriate part of speech. We already know that parts of speech include nouns, verb, adverbs, adjectives, pronouns, conjunction and their sub-categories."
   ]
  },
  {
   "cell_type": "raw",
   "id": "68765aca",
   "metadata": {},
   "source": [
    "Ans.8: Shallow parsing (also chunking or light parsing) is an analysis of a sentence which first identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.). While the most elementary chunking algorithms simply link constituent parts on the basis of elementary search patterns (e.g., as specified by regular expressions), approaches that use machine learning techniques (classifiers, topic modeling, etc.) can take contextual information into account and thus compose chunks in such a way that they better reflect the semantic relations between the basic constituents."
   ]
  },
  {
   "cell_type": "raw",
   "id": "55c480bd",
   "metadata": {},
   "source": [
    "Ans.9: Text chunking is dividing sentences into non-overlapping phrases. Noun phrase chunking deals with extracting the noun phrases from a sentence. While NP chunking is much simpler than parsing, it is still a challenging task to build a accurate and very efficient NP chunker. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cac12426",
   "metadata": {},
   "source": [
    "Ans.10: Named entity recognition (NER)is probably the first step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
